{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:21:06.517238Z","iopub.execute_input":"2024-12-14T13:21:06.517583Z","iopub.status.idle":"2024-12-14T13:21:06.853086Z","shell.execute_reply.started":"2024-12-14T13:21:06.517549Z","shell.execute_reply":"2024-12-14T13:21:06.851981Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s4e12/sample_submission.csv\n/kaggle/input/playground-series-s4e12/train.csv\n/kaggle/input/playground-series-s4e12/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!python -m pip install --upgrade pip\n!python -m pip install autogluon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:21:06.854941Z","iopub.execute_input":"2024-12-14T13:21:06.855419Z","iopub.status.idle":"2024-12-14T13:22:12.570764Z","shell.execute_reply.started":"2024-12-14T13:21:06.855377Z","shell.execute_reply":"2024-12-14T13:22:12.569930Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\nDownloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.0\n    Uninstalling pip-24.0:\n      Successfully uninstalled pip-24.0\nSuccessfully installed pip-24.3.1\nCollecting autogluon\n  Downloading autogluon-1.2-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.core==1.2 (from autogluon.core[all]==1.2->autogluon)\n  Downloading autogluon.core-1.2-py3-none-any.whl.metadata (12 kB)\nCollecting autogluon.features==1.2 (from autogluon)\n  Downloading autogluon.features-1.2-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.tabular==1.2 (from autogluon.tabular[all]==1.2->autogluon)\n  Downloading autogluon.tabular-1.2-py3-none-any.whl.metadata (14 kB)\nCollecting autogluon.multimodal==1.2 (from autogluon)\n  Downloading autogluon.multimodal-1.2-py3-none-any.whl.metadata (12 kB)\nCollecting autogluon.timeseries==1.2 (from autogluon.timeseries[all]==1.2->autogluon)\n  Downloading autogluon.timeseries-1.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy<2.1.4,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.26.4)\nRequirement already satisfied: scipy<1.16,>=1.5.4 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.14.1)\nCollecting scikit-learn<1.5.3,>=1.4.0 (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon)\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.3)\nRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2.2.3)\nRequirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2.32.3)\nRequirement already satisfied: matplotlib<3.11,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.7.5)\nRequirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.26.100)\nCollecting autogluon.common==1.2 (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon)\n  Downloading autogluon.common-1.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: ray<2.40,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (2.24.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core[all]==1.2->autogluon) (17.0.0)\nRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from autogluon.core[all]==1.2->autogluon) (0.2.7)\nRequirement already satisfied: Pillow<12,>=10.0.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (10.3.0)\nRequirement already satisfied: torch<2.6,>=2.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (2.4.0)\nCollecting lightning<2.6,>=2.2 (from autogluon.multimodal==1.2->autogluon)\n  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: transformers<5,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<5,>=4.38.0->autogluon.multimodal==1.2->autogluon) (4.46.3)\nCollecting accelerate<1.0,>=0.34.0 (from autogluon.multimodal==1.2->autogluon)\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nCollecting jsonschema<4.22,>=4.18 (from autogluon.multimodal==1.2->autogluon)\n  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\nCollecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.2->autogluon)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.2->autogluon)\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting timm<1.0.7,>=0.9.5 (from autogluon.multimodal==1.2->autogluon)\n  Downloading timm-1.0.3-py3-none-any.whl.metadata (43 kB)\nRequirement already satisfied: torchvision<0.21.0,>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (0.19.0)\nRequirement already satisfied: scikit-image<0.25.0,>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (0.23.2)\nRequirement already satisfied: text-unidecode<1.4,>=1.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (1.3)\nCollecting torchmetrics<1.3.0,>=1.2.0 (from autogluon.multimodal==1.2->autogluon)\n  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\nCollecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.2->autogluon)\n  Downloading omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)\nCollecting pytorch-metric-learning<2.4,>=1.3.0 (from autogluon.multimodal==1.2->autogluon)\n  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl.metadata (17 kB)\nCollecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.2->autogluon)\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nCollecting nltk<3.9,>=3.4.5 (from autogluon.multimodal==1.2->autogluon)\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nCollecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.2->autogluon)\n  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (0.7.1)\nRequirement already satisfied: jinja2<3.2,>=3.0.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (3.1.4)\nRequirement already satisfied: tensorboard<3,>=2.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (2.16.2)\nCollecting pytesseract<0.3.11,>=0.3.9 (from autogluon.multimodal==1.2->autogluon)\n  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\nCollecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.2->autogluon)\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pdf2image<1.19,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.2->autogluon) (1.17.0)\nRequirement already satisfied: catboost<1.3,>=1.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.2->autogluon) (1.2.7)\nCollecting spacy<3.8 (from autogluon.tabular[all]==1.2->autogluon)\n  Downloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\nRequirement already satisfied: lightgbm<4.6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.2->autogluon) (4.2.0)\nCollecting einops<0.9,>=0.7 (from autogluon.tabular[all]==1.2->autogluon)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: xgboost<2.2,>=1.6 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.2->autogluon) (2.0.3)\nRequirement already satisfied: fastai<2.8,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.2->autogluon) (2.7.18)\nRequirement already satisfied: huggingface-hub[torch] in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.2->autogluon) (0.26.2)\nRequirement already satisfied: joblib<2,>=1.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.4.2)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.4.0)\nCollecting gluonts<0.17,>=0.15.0 (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading gluonts-0.16.0-py3-none-any.whl.metadata (9.8 kB)\nCollecting statsforecast<1.8,>=1.7.0 (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading statsforecast-1.7.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\nCollecting mlforecast==0.13.4 (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading mlforecast-0.13.4-py3-none-any.whl.metadata (12 kB)\nCollecting utilsforecast<0.2.5,>=0.2.3 (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading utilsforecast-0.2.4-py3-none-any.whl.metadata (7.4 kB)\nCollecting coreforecast==0.0.12 (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading coreforecast-0.0.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting fugue>=0.9.0 (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: orjson~=3.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (3.10.4)\nRequirement already satisfied: psutil<7.0.0,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.2->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (5.9.3)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (3.1.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2024.6.0)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.60.0)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (4.1.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (21.3)\nCollecting window-ops (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate<1.0,>=0.34.0->autogluon.multimodal==1.2->autogluon) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate<1.0,>=0.34.0->autogluon.multimodal==1.2->autogluon) (0.4.5)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (0.6.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.2->autogluon) (0.20.3)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.2->autogluon) (5.22.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.2->autogluon) (1.16.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (3.1.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (0.70.16)\nRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.2->autogluon) (24.3.1)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.2->autogluon) (0.0.7)\nRequirement already satisfied: fastcore<1.8,>=1.5.29 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.2->autogluon) (1.7.20)\nRequirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.2->autogluon) (1.0.3)\nCollecting triad>=0.9.7 (from fugue>=0.9.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\nCollecting adagio>=0.2.4 (from fugue>=0.9.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pydantic<3,>=1.7 in /opt/conda/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.10.1)\nRequirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.12.1)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (4.12.2)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.2->autogluon) (1.0.0)\nRequirement already satisfied: py4j in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.2->autogluon) (0.10.9.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.2->autogluon) (2.1.5)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.2->autogluon) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.2->autogluon) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.2->autogluon) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.2->autogluon) (0.18.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.6,>=2.2->autogluon.multimodal==1.2->autogluon) (0.11.9)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2.9.0.post0)\nCollecting gdown>=4.0.0 (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.2->autogluon)\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<3.9,>=3.4.5->autogluon.multimodal==1.2->autogluon) (8.1.7)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<3.9,>=3.4.5->autogluon.multimodal==1.2->autogluon) (2024.5.15)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.2->autogluon)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (0.4.6)\nCollecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon)\n  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\nCollecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon)\n  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (13.7.1)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (0.9.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2024.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray<2.40,>=2.10.0->ray[default]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (3.15.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray<2.40,>=2.10.0->ray[default]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.0.8)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray<2.40,>=2.10.0->ray[default]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (3.20.3)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray<2.40,>=2.10.0->ray[default]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray<2.40,>=2.10.0->ray[default]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.4.1)\nRequirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (2.6.2.2)\nRequirement already satisfied: aiohttp>=3.7 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (3.9.5)\nCollecting aiohttp-cors (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon)\n  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: colorful in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.5.6)\nRequirement already satisfied: py-spy>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.3.14)\nRequirement already satisfied: opencensus in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.11.4)\nRequirement already satisfied: prometheus-client>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.20.0)\nRequirement already satisfied: smart-open in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (7.0.4)\nRequirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (20.21.0)\nRequirement already satisfied: grpcio>=1.42.0 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.62.2)\nRequirement already satisfied: memray in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.12.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2024.6.2)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.25.0,>=0.19.1->autogluon.multimodal==1.2->autogluon) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.25.0,>=0.19.1->autogluon.multimodal==1.2->autogluon) (2024.5.22)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.25.0,>=0.19.1->autogluon.multimodal==1.2->autogluon) (0.4)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.5.3,>=1.4.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.5.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (2.0.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (3.0.9)\nCollecting thinc<8.3.0,>=8.2.2 (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon)\n  Downloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (0.12.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (70.0.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (3.4.1)\nRequirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from statsforecast<1.8,>=1.7.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.14.2)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (1.4.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (3.1.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<2.6,>=2.2->autogluon.multimodal==1.2->autogluon) (1.13.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5,>=4.38.0->transformers[sentencepiece]<5,>=4.38.0->autogluon.multimodal==1.2->autogluon) (0.20.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<5,>=4.38.0->autogluon.multimodal==1.2->autogluon) (0.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (4.0.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.2->autogluon) (4.12.3)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.3.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.43.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.27.1)\nRequirement already satisfied: patsy>=0.5.6 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast<1.8,>=1.7.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.5.6)\nCollecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8->autogluon.tabular[all]==1.2->autogluon)\n  Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (0.1.4)\nCollecting fs (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (2.18.0)\nRequirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.3.8)\nRequirement already satisfied: platformdirs<4,>=2.4 in /opt/conda/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (3.11.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (0.20.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.16.0)\nRequirement already satisfied: textual>=0.41.0 in /opt/conda/lib/python3.10/site-packages (from memray->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.67.1)\nCollecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon)\n  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: opencensus-context>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.1.3)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (2.11.1)\nRequirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (3.21.0)\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon)\n  Downloading openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.14.0)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (6.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.10/site-packages (from optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.0.30)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.2->autogluon) (8.3.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<2.6,>=2.2->autogluon.multimodal==1.2->autogluon) (1.3.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.3.6)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.63.1)\nRequirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (2.30.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.2.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (0.1.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (3.0.3)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.2->autogluon) (2.5)\nRequirement already satisfied: appdirs~=1.4.3 in /opt/conda/lib/python3.10/site-packages (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.4.4)\nCollecting filelock (from ray<2.40,>=2.10.0->ray[default]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon)\n  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon)\n  Downloading oss2-2.17.0.tar.gz (259 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting packaging (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pytz>=2020.1 (from pandas<2.3.0,>=2.0.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon)\n  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\nINFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon)\n  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\nINFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.2->autogluon) (1.7.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (4.9)\nRequirement already satisfied: mdit-py-plugins in /opt/conda/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.4.1)\nRequirement already satisfied: linkify-it-py<3,>=1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (2.0.3)\nRequirement already satisfied: uc-micro-py in /opt/conda/lib/python3.10/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (1.0.3)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (0.6.0)\nDownloading autogluon-1.2-py3-none-any.whl (9.6 kB)\nDownloading autogluon.core-1.2-py3-none-any.whl (266 kB)\nDownloading autogluon.features-1.2-py3-none-any.whl (64 kB)\nDownloading autogluon.multimodal-1.2-py3-none-any.whl (429 kB)\nDownloading autogluon.tabular-1.2-py3-none-any.whl (352 kB)\nDownloading autogluon.timeseries-1.2-py3-none-any.whl (174 kB)\nDownloading autogluon.common-1.2-py3-none-any.whl (68 kB)\nDownloading coreforecast-0.0.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\nDownloading mlforecast-0.13.4-py3-none-any.whl (70 kB)\nDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\nDownloading fugue-0.9.1-py3-none-any.whl (278 kB)\nDownloading gluonts-0.16.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\nDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\nDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\nDownloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\nDownloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\nDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m145.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading statsforecast-1.7.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\nDownloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading utilsforecast-0.2.4-py3-none-any.whl (40 kB)\nDownloading adagio-0.2.6-py3-none-any.whl (19 kB)\nDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nDownloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.4/922.4 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triad-0.9.8-py3-none-any.whl (62 kB)\nDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\nDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\nDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\nDownloading window_ops-0.0.15-py3-none-any.whl (15 kB)\nDownloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m134.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\nDownloading openxlab-0.0.11-py3-none-any.whl (55 kB)\nDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\nBuilding wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=633b095f07b078f54f5b471487536bffed1ed12e63c347936a02d79b7b87843b\n  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=3a732f206c922a70d881120e4139df325260500f0243892c7bf49c2a5580f6c9\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=fe46f6caf00d052f487d0a504c4bce2d01b08206bca36ffc643594905d325f19\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\nInstalling collected packages: nvidia-ml-py3, antlr4-python3-runtime, ordered-set, openxlab, omegaconf, nltk, fs, einops, coreforecast, blis, window-ops, scikit-learn, pytesseract, model-index, botocore, utilsforecast, triad, torchmetrics, seqeval, pytorch-metric-learning, opendatalab, jsonschema, gluonts, gdown, aiohttp-cors, accelerate, timm, thinc, openmim, nlpaug, mlforecast, adagio, spacy, lightning, fugue, evaluate, autogluon.common, statsforecast, autogluon.features, autogluon.core, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: blis\n    Found existing installation: blis 1.0.1\n    Uninstalling blis-1.0.1:\n      Successfully uninstalled blis-1.0.1\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: pytesseract\n    Found existing installation: pytesseract 0.3.13\n    Uninstalling pytesseract-0.3.13:\n      Successfully uninstalled pytesseract-0.3.13\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.35.36\n    Uninstalling botocore-1.35.36:\n      Successfully uninstalled botocore-1.35.36\n  Attempting uninstall: torchmetrics\n    Found existing installation: torchmetrics 1.6.0\n    Uninstalling torchmetrics-1.6.0:\n      Successfully uninstalled torchmetrics-1.6.0\n  Attempting uninstall: jsonschema\n    Found existing installation: jsonschema 4.22.0\n    Uninstalling jsonschema-4.22.0:\n      Successfully uninstalled jsonschema-4.22.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.1.1\n    Uninstalling accelerate-1.1.1:\n      Successfully uninstalled accelerate-1.1.1\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.11\n    Uninstalling timm-1.0.11:\n      Successfully uninstalled timm-1.0.11\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.3.2\n    Uninstalling thinc-8.3.2:\n      Successfully uninstalled thinc-8.3.2\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.8.2\n    Uninstalling spacy-3.8.2:\n      Successfully uninstalled spacy-3.8.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.15.2 requires botocore<1.35.37,>=1.35.16, but you have botocore 1.29.165 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.3 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.10.1 which is incompatible.\njupyterlab 4.3.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.34.2 adagio-0.2.6 aiohttp-cors-0.7.0 antlr4-python3-runtime-4.9.3 autogluon-1.2 autogluon.common-1.2 autogluon.core-1.2 autogluon.features-1.2 autogluon.multimodal-1.2 autogluon.tabular-1.2 autogluon.timeseries-1.2 blis-0.7.11 botocore-1.29.165 coreforecast-0.0.12 einops-0.8.0 evaluate-0.4.3 fs-2.4.16 fugue-0.9.1 gdown-5.2.0 gluonts-0.16.0 jsonschema-4.21.1 lightning-2.4.0 mlforecast-0.13.4 model-index-0.1.11 nlpaug-1.1.11 nltk-3.8.1 nvidia-ml-py3-7.352.0 omegaconf-2.2.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 ordered-set-4.1.0 pytesseract-0.3.10 pytorch-metric-learning-2.3.0 scikit-learn-1.5.2 seqeval-1.2.2 spacy-3.7.5 statsforecast-1.7.8 thinc-8.2.5 timm-1.0.3 torchmetrics-1.2.1 triad-0.9.8 utilsforecast-0.2.4 window-ops-0.0.15\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import root_mean_squared_log_error\nfrom autogluon.core.metrics import make_scorer\n\nroot_mean_squared_log_scorer = make_scorer(name='root_mean_squared_log_error',\n                                 score_func=root_mean_squared_log_error,\n                                 optimum=0,\n                                 greater_is_better=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:22:12.572183Z","iopub.execute_input":"2024-12-14T13:22:12.572600Z","iopub.status.idle":"2024-12-14T13:22:13.023134Z","shell.execute_reply.started":"2024-12-14T13:22:12.572548Z","shell.execute_reply":"2024-12-14T13:22:13.022400Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from autogluon.tabular import TabularDataset, TabularPredictor\n\ntrain_data = TabularDataset('/kaggle/input/playground-series-s4e12/train.csv')\ntrain_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:22:13.025045Z","iopub.execute_input":"2024-12-14T13:22:13.025556Z","iopub.status.idle":"2024-12-14T13:22:19.550760Z","shell.execute_reply.started":"2024-12-14T13:22:13.025520Z","shell.execute_reply":"2024-12-14T13:22:19.549867Z"}},"outputs":[{"name":"stderr","text":"Loaded data from: /kaggle/input/playground-series-s4e12/train.csv | Columns = 21 / 21 | Rows = 1200000 -> 1200000\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id   Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n0   0  19.0  Female        10049.0        Married                   1.0   \n1   1  39.0  Female        31678.0       Divorced                   3.0   \n2   2  23.0    Male        25602.0       Divorced                   3.0   \n3   3  21.0    Male       141855.0        Married                   2.0   \n4   4  21.0    Male        39651.0         Single                   1.0   \n\n  Education Level     Occupation  Health Score  Location  ... Previous Claims  \\\n0      Bachelor's  Self-Employed     22.598761     Urban  ...             2.0   \n1        Master's            NaN     15.569731     Rural  ...             1.0   \n2     High School  Self-Employed     47.177549  Suburban  ...             1.0   \n3      Bachelor's            NaN     10.938144     Rural  ...             1.0   \n4      Bachelor's  Self-Employed     20.376094     Rural  ...             0.0   \n\n   Vehicle Age  Credit Score  Insurance Duration           Policy Start Date  \\\n0         17.0         372.0                 5.0  2023-12-23 15:21:39.134960   \n1         12.0         694.0                 2.0  2023-06-12 15:21:39.111551   \n2         14.0           NaN                 3.0  2023-09-30 15:21:39.221386   \n3          0.0         367.0                 1.0  2024-06-12 15:21:39.226954   \n4          8.0         598.0                 4.0  2021-12-01 15:21:39.252145   \n\n  Customer Feedback Smoking Status Exercise Frequency Property Type  \\\n0              Poor             No             Weekly         House   \n1           Average            Yes            Monthly         House   \n2              Good            Yes             Weekly         House   \n3              Poor            Yes              Daily     Apartment   \n4              Poor            Yes             Weekly         House   \n\n  Premium Amount  \n0         2869.0  \n1         1483.0  \n2          567.0  \n3          765.0  \n4         2022.0  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Annual Income</th>\n      <th>Marital Status</th>\n      <th>Number of Dependents</th>\n      <th>Education Level</th>\n      <th>Occupation</th>\n      <th>Health Score</th>\n      <th>Location</th>\n      <th>...</th>\n      <th>Previous Claims</th>\n      <th>Vehicle Age</th>\n      <th>Credit Score</th>\n      <th>Insurance Duration</th>\n      <th>Policy Start Date</th>\n      <th>Customer Feedback</th>\n      <th>Smoking Status</th>\n      <th>Exercise Frequency</th>\n      <th>Property Type</th>\n      <th>Premium Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>19.0</td>\n      <td>Female</td>\n      <td>10049.0</td>\n      <td>Married</td>\n      <td>1.0</td>\n      <td>Bachelor's</td>\n      <td>Self-Employed</td>\n      <td>22.598761</td>\n      <td>Urban</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>17.0</td>\n      <td>372.0</td>\n      <td>5.0</td>\n      <td>2023-12-23 15:21:39.134960</td>\n      <td>Poor</td>\n      <td>No</td>\n      <td>Weekly</td>\n      <td>House</td>\n      <td>2869.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>39.0</td>\n      <td>Female</td>\n      <td>31678.0</td>\n      <td>Divorced</td>\n      <td>3.0</td>\n      <td>Master's</td>\n      <td>NaN</td>\n      <td>15.569731</td>\n      <td>Rural</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>12.0</td>\n      <td>694.0</td>\n      <td>2.0</td>\n      <td>2023-06-12 15:21:39.111551</td>\n      <td>Average</td>\n      <td>Yes</td>\n      <td>Monthly</td>\n      <td>House</td>\n      <td>1483.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>23.0</td>\n      <td>Male</td>\n      <td>25602.0</td>\n      <td>Divorced</td>\n      <td>3.0</td>\n      <td>High School</td>\n      <td>Self-Employed</td>\n      <td>47.177549</td>\n      <td>Suburban</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>2023-09-30 15:21:39.221386</td>\n      <td>Good</td>\n      <td>Yes</td>\n      <td>Weekly</td>\n      <td>House</td>\n      <td>567.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>21.0</td>\n      <td>Male</td>\n      <td>141855.0</td>\n      <td>Married</td>\n      <td>2.0</td>\n      <td>Bachelor's</td>\n      <td>NaN</td>\n      <td>10.938144</td>\n      <td>Rural</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>367.0</td>\n      <td>1.0</td>\n      <td>2024-06-12 15:21:39.226954</td>\n      <td>Poor</td>\n      <td>Yes</td>\n      <td>Daily</td>\n      <td>Apartment</td>\n      <td>765.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>21.0</td>\n      <td>Male</td>\n      <td>39651.0</td>\n      <td>Single</td>\n      <td>1.0</td>\n      <td>Bachelor's</td>\n      <td>Self-Employed</td>\n      <td>20.376094</td>\n      <td>Rural</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>598.0</td>\n      <td>4.0</td>\n      <td>2021-12-01 15:21:39.252145</td>\n      <td>Poor</td>\n      <td>Yes</td>\n      <td>Weekly</td>\n      <td>House</td>\n      <td>2022.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"label = 'Premium Amount'\ntrain_data[label].describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:22:19.551811Z","iopub.execute_input":"2024-12-14T13:22:19.552109Z","iopub.status.idle":"2024-12-14T13:22:19.608044Z","shell.execute_reply.started":"2024-12-14T13:22:19.552081Z","shell.execute_reply":"2024-12-14T13:22:19.606999Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"count    1.200000e+06\nmean     1.102545e+03\nstd      8.649989e+02\nmin      2.000000e+01\n25%      5.140000e+02\n50%      8.720000e+02\n75%      1.509000e+03\nmax      4.999000e+03\nName: Premium Amount, dtype: float64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"time_limit = 21600\nmetric = root_mean_squared_log_scorer \npredictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:22:19.610111Z","iopub.execute_input":"2024-12-14T13:22:19.610499Z"}},"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20241214_132219\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.2\nPython Version:     3.10.14\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.53 GB / 31.35 GB (94.2%)\nDisk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n===================================================\nPresets specified: ['best_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 5400s of the 21600s of remaining time (25%).\n\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n2024-12-14 13:22:24,634\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20241214_132219/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=254)\u001b[0m Running DyStack sub-fit ...\n\u001b[36m(_dystack pid=254)\u001b[0m Beginning AutoGluon training ... Time limit = 5394s\n\u001b[36m(_dystack pid=254)\u001b[0m AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20241214_132219/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=254)\u001b[0m Train Data Rows:    1066666\n\u001b[36m(_dystack pid=254)\u001b[0m Train Data Columns: 20\n\u001b[36m(_dystack pid=254)\u001b[0m Label Column:       Premium Amount\n\u001b[36m(_dystack pid=254)\u001b[0m Problem Type:       regression\n\u001b[36m(_dystack pid=254)\u001b[0m Preprocessing data ...\n\u001b[36m(_dystack pid=254)\u001b[0m Using Feature Generators to preprocess the data ...\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \tAvailable Memory:                    29959.13 MB\n\u001b[36m(_dystack pid=254)\u001b[0m \tTrain Data (Original)  Memory Usage: 789.87 MB (2.6% of available memory)\n\u001b[36m(_dystack pid=254)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\u001b[36m(_dystack pid=254)\u001b[0m \tStage 1 Generators:\n\u001b[36m(_dystack pid=254)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n\u001b[36m(_dystack pid=254)\u001b[0m \tStage 2 Generators:\n\u001b[36m(_dystack pid=254)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \tStage 3 Generators:\n\u001b[36m(_dystack pid=254)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \t\tFitting DatetimeFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \tStage 4 Generators:\n\u001b[36m(_dystack pid=254)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \tStage 5 Generators:\n\u001b[36m(_dystack pid=254)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n\u001b[36m(_dystack pid=254)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('float', [])                      :  8 | ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', 'Previous Claims', ...]\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('int', [])                        :  1 | ['id']\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('object', [])                     : 10 | ['Gender', 'Marital Status', 'Education Level', 'Occupation', 'Location', ...]\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('object', ['datetime_as_object']) :  1 | ['Policy Start Date']\n\u001b[36m(_dystack pid=254)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('category', [])             : 8 | ['Marital Status', 'Education Level', 'Occupation', 'Location', 'Policy Type', ...]\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('float', [])                : 8 | ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', 'Previous Claims', ...]\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('int', [])                  : 1 | ['id']\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('int', ['bool'])            : 2 | ['Gender', 'Smoking Status']\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t('int', ['datetime_as_int']) : 5 | ['Policy Start Date', 'Policy Start Date.year', 'Policy Start Date.month', 'Policy Start Date.day', 'Policy Start Date.dayofweek']\n\u001b[36m(_dystack pid=254)\u001b[0m \t8.0s = Fit runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t20 features in original data used to generate 24 features in processed data.\n\u001b[36m(_dystack pid=254)\u001b[0m \tTrain Data (Processed) Memory Usage: 124.11 MB (0.4% of available memory)\n\u001b[36m(_dystack pid=254)\u001b[0m Data preprocessing and feature engineering runtime = 8.61s ...\n\u001b[36m(_dystack pid=254)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_log_error'\n\u001b[36m(_dystack pid=254)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\u001b[36m(_dystack pid=254)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n\u001b[36m(_dystack pid=254)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n\u001b[36m(_dystack pid=254)\u001b[0m User-specified model hyperparameters to be fit:\n\u001b[36m(_dystack pid=254)\u001b[0m {\n\u001b[36m(_dystack pid=254)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\u001b[36m(_dystack pid=254)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\u001b[36m(_dystack pid=254)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\u001b[36m(_dystack pid=254)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\u001b[36m(_dystack pid=254)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\u001b[36m(_dystack pid=254)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=254)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=254)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n\u001b[36m(_dystack pid=254)\u001b[0m }\n\u001b[36m(_dystack pid=254)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3589.26s of the 5385.23s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.2053\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t2.54s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t8.11s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3577.92s of the 5373.89s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.2157\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t1.99s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t8.05s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3567.19s of the 5363.16s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.92%)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=640)\u001b[0m [1000]\tvalid_set's l2: 701713\tvalid_set's root_mean_squared_log_error: -1.13312\n\u001b[36m(_ray_fit pid=639)\u001b[0m [2000]\tvalid_set's l2: 708392\tvalid_set's root_mean_squared_log_error: -1.13869\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n\u001b[36m(_ray_fit pid=637)\u001b[0m [3000]\tvalid_set's l2: 698743\tvalid_set's root_mean_squared_log_error: -1.13462\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=637)\u001b[0m [4000]\tvalid_set's l2: 698705\tvalid_set's root_mean_squared_log_error: -1.13422\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=639)\u001b[0m [5000]\tvalid_set's l2: 708564\tvalid_set's root_mean_squared_log_error: -1.13773\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=254)\u001b[0m \tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training... Skipping this model.\n\u001b[36m(_dystack pid=254)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=638, ip=172.19.2.2)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n\u001b[36m(_dystack pid=254)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n\u001b[36m(_dystack pid=254)\u001b[0m     out = self._fit(**kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n\u001b[36m(_dystack pid=254)\u001b[0m     self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n\u001b[36m(_dystack pid=254)\u001b[0m     return lgb.train(**train_params)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py\", line 283, in train\n\u001b[36m(_dystack pid=254)\u001b[0m     evaluation_result_list.extend(booster.eval_valid(feval))\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4159, in eval_valid\n\u001b[36m(_dystack pid=254)\u001b[0m     return [item for i in range(1, self.__num_dataset)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4160, in <listcomp>\n\u001b[36m(_dystack pid=254)\u001b[0m     for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4868, in __inner_eval\n\u001b[36m(_dystack pid=254)\u001b[0m     feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 48, in function_template\n\u001b[36m(_dystack pid=254)\u001b[0m     return metric.name, compute(y_true, y_hat), is_higher_better\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 97, in __call__\n\u001b[36m(_dystack pid=254)\u001b[0m     return self._score(y_true=y_true, y_pred=y_pred, **k)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 135, in _score\n\u001b[36m(_dystack pid=254)\u001b[0m     return self._sign * self._score_func(y_true, y_pred, **kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n\u001b[36m(_dystack pid=254)\u001b[0m     return func(*args, **kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 768, in root_mean_squared_log_error\n\u001b[36m(_dystack pid=254)\u001b[0m     raise ValueError(\n\u001b[36m(_dystack pid=254)\u001b[0m ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n\u001b[36m(_dystack pid=254)\u001b[0m Detailed Traceback:\n\u001b[36m(_dystack pid=254)\u001b[0m Traceback (most recent call last):\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n\u001b[36m(_dystack pid=254)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n\u001b[36m(_dystack pid=254)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n\u001b[36m(_dystack pid=254)\u001b[0m     out = self._fit(**kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n\u001b[36m(_dystack pid=254)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n\u001b[36m(_dystack pid=254)\u001b[0m     self._fit_folds(\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n\u001b[36m(_dystack pid=254)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n\u001b[36m(_dystack pid=254)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n\u001b[36m(_dystack pid=254)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n\u001b[36m(_dystack pid=254)\u001b[0m     raise processed_exception\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n\u001b[36m(_dystack pid=254)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n\u001b[36m(_dystack pid=254)\u001b[0m     return fn(*args, **kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n\u001b[36m(_dystack pid=254)\u001b[0m     return func(*args, **kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2613, in get\n\u001b[36m(_dystack pid=254)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 861, in get_objects\n\u001b[36m(_dystack pid=254)\u001b[0m     raise value.as_instanceof_cause()\n\u001b[36m(_dystack pid=254)\u001b[0m ray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=638, ip=172.19.2.2)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n\u001b[36m(_dystack pid=254)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n\u001b[36m(_dystack pid=254)\u001b[0m     out = self._fit(**kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n\u001b[36m(_dystack pid=254)\u001b[0m     self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n\u001b[36m(_dystack pid=254)\u001b[0m     return lgb.train(**train_params)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py\", line 283, in train\n\u001b[36m(_dystack pid=254)\u001b[0m     evaluation_result_list.extend(booster.eval_valid(feval))\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4159, in eval_valid\n\u001b[36m(_dystack pid=254)\u001b[0m     return [item for i in range(1, self.__num_dataset)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4160, in <listcomp>\n\u001b[36m(_dystack pid=254)\u001b[0m     for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4868, in __inner_eval\n\u001b[36m(_dystack pid=254)\u001b[0m     feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 48, in function_template\n\u001b[36m(_dystack pid=254)\u001b[0m     return metric.name, compute(y_true, y_hat), is_higher_better\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 97, in __call__\n\u001b[36m(_dystack pid=254)\u001b[0m     return self._score(y_true=y_true, y_pred=y_pred, **k)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 135, in _score\n\u001b[36m(_dystack pid=254)\u001b[0m     return self._sign * self._score_func(y_true, y_pred, **kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n\u001b[36m(_dystack pid=254)\u001b[0m     return func(*args, **kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 768, in root_mean_squared_log_error\n\u001b[36m(_dystack pid=254)\u001b[0m     raise ValueError(\n\u001b[36m(_dystack pid=254)\u001b[0m ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 2972.57s of the 4768.54s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.95%)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(_ray_fit pid=760)\u001b[0m [1000]\tvalid_set's l2: 694714\tvalid_set's root_mean_squared_log_error: -1.13081\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_ray_fit pid=1072)\u001b[0m [1000]\tvalid_set's l2: 696774\tvalid_set's root_mean_squared_log_error: -1.12803\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(_dystack pid=254)\u001b[0m \t-1.1311\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t209.21s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t44.65s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2754.66s of the 4550.63s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.1284\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t1555.06s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t36.82s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 1161.20s of the 2957.17s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.99%)\n\u001b[36m(_ray_fit pid=1411)\u001b[0m \tRan out of time, early stopping on iteration 189.\n\u001b[36m(_ray_fit pid=1552)\u001b[0m \tRan out of time, early stopping on iteration 189.\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.1412\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t926.84s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t0.95s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 230.68s of the 2026.65s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 127 due to low time. Expected time usage reduced from 539.7s -> 230.0s...\n\u001b[36m(_ray_fit pid=1660)\u001b[0m \tRan out of time, early stopping on iteration 192.\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=254)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 101.87s compared to 76.4s of available time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tTime limit exceeded... Skipping ExtraTreesMSE_BAG_L1.\n\u001b[36m(_dystack pid=254)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n\u001b[36m(_dystack pid=254)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n\u001b[36m(_dystack pid=254)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 7.15s of the 1803.12s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=5.26%)\n\u001b[36m(_ray_fit pid=1942)\u001b[0m Metric root_mean_squared_log_error is not supported by this model - using mean_squared_error instead\n\u001b[36m(_dystack pid=254)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1788.31s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.1284\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t2.83s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t0.02s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1785.40s of the 1785.28s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.49%)\n\u001b[36m(_ray_fit pid=1941)\u001b[0m Metric root_mean_squared_log_error is not supported by this model - using mean_squared_error instead\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.1288\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t124.02s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t16.13s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 1655.26s of the 1655.14s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.48%)\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.1286\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t183.81s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t19.84s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1463.90s of the 1463.78s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 134 due to low time. Expected time usage reduced from 3274.1s -> 1463.0s...\n\u001b[36m(_dystack pid=254)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1943, ip=172.19.2.2)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n\u001b[36m(_dystack pid=254)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n\u001b[36m(_dystack pid=254)\u001b[0m     out = self._fit(**kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 338, in _fit\n\u001b[36m(_dystack pid=254)\u001b[0m     raise TimeLimitExceeded\n\u001b[36m(_dystack pid=254)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n\u001b[36m(_dystack pid=254)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1942, ip=172.19.2.2)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n\u001b[36m(_dystack pid=254)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n\u001b[36m(_dystack pid=254)\u001b[0m     out = self._fit(**kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 338, in _fit\n\u001b[36m(_dystack pid=254)\u001b[0m     raise TimeLimitExceeded\n\u001b[36m(_dystack pid=254)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n\u001b[36m(_dystack pid=254)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1941, ip=172.19.2.2)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n\u001b[36m(_dystack pid=254)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n\u001b[36m(_dystack pid=254)\u001b[0m     out = self._fit(**kwargs)\n\u001b[36m(_dystack pid=254)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 338, in _fit\n\u001b[36m(_dystack pid=254)\u001b[0m     raise TimeLimitExceeded\n\u001b[36m(_dystack pid=254)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.1286\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t1452.91s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t18.82s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -9.05s of remaining time.\n\u001b[36m(_dystack pid=254)\u001b[0m \tEnsemble Weights: {'RandomForestMSE_BAG_L1': 0.6, 'RandomForestMSE_BAG_L2': 0.4}\n\u001b[36m(_dystack pid=254)\u001b[0m \t-1.1283\t = Validation score   (-root_mean_squared_log_error)\n\u001b[36m(_dystack pid=254)\u001b[0m \t3.09s\t = Training   runtime\n\u001b[36m(_dystack pid=254)\u001b[0m \t0.02s\t = Validation runtime\n\u001b[36m(_dystack pid=254)\u001b[0m AutoGluon training complete, total runtime = 5406.32s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2443.0 rows/s (133334 batch size)\n\u001b[36m(_dystack pid=254)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20241214_132219/ds_sub_fit/sub_fit_ho\")\n\u001b[36m(_dystack pid=254)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                    model  score_holdout  score_val                  eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0     WeightedEnsemble_L3      -1.131416  -1.128264  root_mean_squared_log_error       21.245191     117.418577  4151.644587                 0.004342                0.020001           3.088864            3       True         10\n1  RandomForestMSE_BAG_L1      -1.131524  -1.128440  root_mean_squared_log_error        3.449199      36.818878  1555.064983                 3.449199               36.818878        1555.064983            1       True          4\n2     WeightedEnsemble_L2      -1.131524  -1.128440  root_mean_squared_log_error        3.451683      36.837575  1557.890947                 0.002484                0.018697           2.825964            2       True          6\n3  RandomForestMSE_BAG_L2      -1.131557  -1.128587  root_mean_squared_log_error       21.240849     117.398576  4148.555723                 1.756321               18.815372        1452.908887            2       True          9\n4         LightGBM_BAG_L2      -1.131979  -1.128585  root_mean_squared_log_error       25.286321     118.425009  2879.460442                 5.801792               19.841805         183.813606            2       True          8\n5       LightGBMXT_BAG_L2      -1.132249  -1.128774  root_mean_squared_log_error       23.972031     114.711397  2819.669699                 4.487503               16.128193         124.022864            2       True          7\n6         LightGBM_BAG_L1      -1.133873  -1.131110  root_mean_squared_log_error       12.906354      44.654280   209.213714                12.906354               44.654280         209.213714            1       True          3\n7         CatBoost_BAG_L1      -1.143496  -1.141218  root_mean_squared_log_error        0.777395       0.947576   926.835629                 0.777395                0.947576         926.835629            1       True          5\n8   KNeighborsUnif_BAG_L1      -1.203907  -1.205339  root_mean_squared_log_error        1.145190       8.113410     2.538142                 1.145190                8.113410           2.538142            1       True          1\n9   KNeighborsDist_BAG_L1      -1.214300  -1.215704  root_mean_squared_log_error        1.206390       8.049060     1.994368                 1.206390                8.049060           1.994368            1       True          2\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t5449s\t = DyStack   runtime |\t16151s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 16151s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20241214_132219\"\nTrain Data Rows:    1200000\nTrain Data Columns: 20\nLabel Column:       Premium Amount\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30040.47 MB\n\tTrain Data (Original)  Memory Usage: 888.65 MB (3.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\t\tFitting DatetimeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', [])                      :  8 | ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', 'Previous Claims', ...]\n\t\t('int', [])                        :  1 | ['id']\n\t\t('object', [])                     : 10 | ['Gender', 'Marital Status', 'Education Level', 'Occupation', 'Location', ...]\n\t\t('object', ['datetime_as_object']) :  1 | ['Policy Start Date']\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])             : 8 | ['Marital Status', 'Education Level', 'Occupation', 'Location', 'Policy Type', ...]\n\t\t('float', [])                : 8 | ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', 'Previous Claims', ...]\n\t\t('int', [])                  : 1 | ['id']\n\t\t('int', ['bool'])            : 2 | ['Gender', 'Smoking Status']\n\t\t('int', ['datetime_as_int']) : 5 | ['Policy Start Date', 'Policy Start Date.year', 'Policy Start Date.month', 'Policy Start Date.day', 'Policy Start Date.dayofweek']\n\t8.7s = Fit runtime\n\t20 features in original data used to generate 24 features in processed data.\n\tTrain Data (Processed) Memory Usage: 139.62 MB (0.5% of available memory)\nData preprocessing and feature engineering runtime = 9.25s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_log_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 108 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 10758.75s of the 16142.14s of remaining time.\n\t-1.2049\t = Validation score   (-root_mean_squared_log_error)\n\t2.09s\t = Training   runtime\n\t9.49s\t = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ... Training model for up to 10746.38s of the 16129.78s of remaining time.\n\t-1.2152\t = Validation score   (-root_mean_squared_log_error)\n\t1.89s\t = Training   runtime\n\t9.33s\t = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 10734.43s of the 16117.83s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.04%)\n\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training... Skipping this model.\n\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=3212, ip=172.19.2.2)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n    out = self._fit(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n    return lgb.train(**train_params)\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py\", line 283, in train\n    evaluation_result_list.extend(booster.eval_valid(feval))\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4159, in eval_valid\n    return [item for i in range(1, self.__num_dataset)\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4160, in <listcomp>\n    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4868, in __inner_eval\n    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 48, in function_template\n    return metric.name, compute(y_true, y_hat), is_higher_better\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 97, in __call__\n    return self._score(y_true=y_true, y_pred=y_pred, **k)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 135, in _score\n    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 768, in root_mean_squared_log_error\n    raise ValueError(\nValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\nDetailed Traceback:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n    model = self._train_single(**model_fit_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n    out = self._fit(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n    self._fit_folds(\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n    fold_fitting_strategy.after_all_folds_scheduled()\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n    self._process_fold_results(finished, unfinished, fold_ctx)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n    raise processed_exception\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2613, in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 861, in get_objects\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=3212, ip=172.19.2.2)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n    out = self._fit(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n    return lgb.train(**train_params)\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py\", line 283, in train\n    evaluation_result_list.extend(booster.eval_valid(feval))\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4159, in eval_valid\n    return [item for i in range(1, self.__num_dataset)\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4160, in <listcomp>\n    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 4868, in __inner_eval\n    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 48, in function_template\n    return metric.name, compute(y_true, y_hat), is_higher_better\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 97, in __call__\n    return self._score(y_true=y_true, y_pred=y_pred, **k)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 135, in _score\n    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 768, in root_mean_squared_log_error\n    raise ValueError(\nValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\nFitting model: LightGBM_BAG_L1 ... Training model for up to 10353.57s of the 15736.96s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.06%)\n2024-12-14 15:00:08,731\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n2024-12-14 15:00:08,733\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n2024-12-14 15:00:09,731\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n\t-1.1312\t = Validation score   (-root_mean_squared_log_error)\n\t153.63s\t = Training   runtime\n\t23.98s\t = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 10192.10s of the 15575.49s of remaining time.\n\t-1.1286\t = Validation score   (-root_mean_squared_log_error)\n\t1747.24s\t = Training   runtime\n\t40.18s\t = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 8403.08s of the 13786.48s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.12%)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"test_data = TabularDataset(f'/kaggle/input/playground-series-s4e12/test.csv')\ntest_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = predictor.predict(test_data)\ny_pred.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = TabularDataset(f'/kaggle/input/playground-series-s4e12/sample_submission.csv')\nsample = sample.drop(columns=[label])\nsample[label] = y_pred\nsample.head()\nsample.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}